{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4995c060",
   "metadata": {},
   "source": [
    "# Weeks 7 and 8. Machine learning\n",
    "# Part 1. Classification\n",
    "Machine learning is a very general term that covers many parts of data science. Here, we will look at two specific problems that machine learning is well equipped to handle:\n",
    "* Classification problems (this notebook)\n",
    "* Clustering (next notebook)\n",
    "\n",
    "As a broad generalization, machine learning-based classification focuses on *prediction*. For example: [which neighborhoods are likely to gentrify](https://journals.sagepub.com/doi/abs/10.1177/0042098018789054)? [Which facilities are likely to be violating environmental standards?](https://www.nature.com/articles/s41893-018-0142-9) What is demand likely to be at a new bikeshare station? [What is the race and gender of an author on a course reading list](http://syllabusdiversity.org)?\n",
    "\n",
    "There are also applications which raise more concerns with ethics and justice (yes, [predictive policing](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/), I'm talking about you). We'll come back to these issues in a couple of weeks.\n",
    "\n",
    "Machine learning is less successful with questions of *causation* and *hypothesis testing*. Here, a statistical approach (frequentist or Bayesian) is likely to be more appropriate, although there is quite a bit of overlap between \"statistics\" and \"machine learning.\"\n",
    "\n",
    "There are at least three widely used approaches to classification.\n",
    "* Logistic regression. This is often used in a more statistical setting, but is the starting point for much machine learning analysis. \n",
    "* Random forests. We'll focus on this technique.\n",
    "* Neural networks. Often used for image recognition, this can be a \"black box\" approach to prediction and classification.\n",
    "\n",
    "Important: machine learning is a very large field, and there are entire courses on the theory and applications. Here, we will give a very high-level overview. We'll focus on the big-picture applicability of machine learning techniques, and actually implementing them in Python. We'll skate over the theoretical underpinnings and the details of the various algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6ef63",
   "metadata": {},
   "source": [
    "## Example: ADUs in LA\n",
    "The example we will use is whether property owners construct Accessory Dwelling Units (ADUs) in the City of Los Angeles. You might imagine that a predictive approach could be useful to planners and policymakers. Not least, they could predict future ADU growth, and the neighborhoods where ADUs are most likely to be built.\n",
    "\n",
    "We can obtain the data from the City's building permits database (which tells us whether or not an ADU was built), and the County Assessor parcel database (which provides covariates such as lot size). Because both of these datasets are very large, I preprocessed them and saved a slimmed-down version that is in your GitHub folder. Specifically, I extracted a subset of fields, limited the building permits to those that include an ADU, and limited the parcels to those in the City of LA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1637078",
   "metadata": {},
   "source": [
    "## Wrangling the data\n",
    "We have two input data files: permits and parcels. The aim: add a column to the parcels dataframe that is `True` if an ADU has been permitted on that parcel, and `False` otherwise.\n",
    "    \n",
    "Even with this preprocessing, there is some work to do in joining the datasets together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e13cb46",
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assessor Book</th>\n",
       "      <th>Assessor Page</th>\n",
       "      <th>Assessor Parcel</th>\n",
       "      <th># of Accessory Dwelling Units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2340.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>013</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5535.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>028</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4249.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Assessor Book  Assessor Page Assessor Parcel  # of Accessory Dwelling Units\n",
       "0         2340.0           20.0             013                            1.0\n",
       "1         5535.0           34.0             001                            1.0\n",
       "2         2639.0           23.0             005                            1.0\n",
       "3         2276.0           18.0             028                            1.0\n",
       "4         4249.0            6.0             016                            1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import pandas as pd\n",
    "\n",
    "# get building permit data\n",
    "# this is an abbreviated version of the data here (>500 MB):\n",
    "# https://data.lacity.org/City-Infrastructure-Service-Requests/Building-and-Safety-Permit-Information-Old/yv23-pmwf\n",
    "\n",
    "# this code was used to read in the data and save a subset (ADU permits only) that is manageable in size\n",
    "if 0:  # if 0 means this block won't be executed\n",
    "    cols_to_use = ['Assessor Book', 'Assessor Page', 'Assessor Parcel', '# of Accessory Dwelling Units']\n",
    "    df = pd.read_csv('/Users/adammb/Desktop/Building_and_Safety_Permit_Information_Old.csv', usecols=cols_to_use)\n",
    "    df = df[df['# of Accessory Dwelling Units']>0]\n",
    "    df.to_csv('ADU_permits.csv', index=False)\n",
    "    del df\n",
    "\n",
    "permits = pd.read_csv('ADU_permits.csv')  # this file should be in your GitHub folder\n",
    "permits.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c50b4d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APN</th>\n",
       "      <th>UseType</th>\n",
       "      <th>UseDescription</th>\n",
       "      <th>YearBuilt1</th>\n",
       "      <th>Units1</th>\n",
       "      <th>Bedrooms1</th>\n",
       "      <th>Bathrooms1</th>\n",
       "      <th>SQFTmain1</th>\n",
       "      <th>Roll_LandValue</th>\n",
       "      <th>Roll_ImpValue</th>\n",
       "      <th>Roll_LandBaseYear</th>\n",
       "      <th>Roll_ImpBaseYear</th>\n",
       "      <th>CENTER_LAT</th>\n",
       "      <th>CENTER_LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-001-003</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2090.0</td>\n",
       "      <td>543000.0</td>\n",
       "      <td>231000.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006</td>\n",
       "      <td>34.220220</td>\n",
       "      <td>-118.620669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-001-004</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2479.0</td>\n",
       "      <td>345587.0</td>\n",
       "      <td>238650.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>34.220039</td>\n",
       "      <td>-118.620668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-001-005</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>490917.0</td>\n",
       "      <td>185207.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>34.219858</td>\n",
       "      <td>-118.620676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-001-008</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2423.0</td>\n",
       "      <td>119775.0</td>\n",
       "      <td>207020.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>34.220334</td>\n",
       "      <td>-118.622706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-001-009</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>130511.0</td>\n",
       "      <td>195871.0</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>34.220323</td>\n",
       "      <td>-118.623050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            APN      UseType UseDescription  YearBuilt1  Units1  Bedrooms1  \\\n",
       "0  2004-001-003  Residential         Single      1973.0     1.0        4.0   \n",
       "1  2004-001-004  Residential         Single      1973.0     1.0        5.0   \n",
       "2  2004-001-005  Residential         Single      1973.0     1.0        4.0   \n",
       "3  2004-001-008  Residential         Single      1978.0     1.0        4.0   \n",
       "4  2004-001-009  Residential         Single      1978.0     1.0        4.0   \n",
       "\n",
       "   Bathrooms1  SQFTmain1  Roll_LandValue  Roll_ImpValue  Roll_LandBaseYear  \\\n",
       "0         3.0     2090.0        543000.0       231000.0               2006   \n",
       "1         3.0     2479.0        345587.0       238650.0               2010   \n",
       "2         2.0     2057.0        490917.0       185207.0               2018   \n",
       "3         3.0     2423.0        119775.0       207020.0               1980   \n",
       "4         3.0     2226.0        130511.0       195871.0               1984   \n",
       "\n",
       "   Roll_ImpBaseYear  CENTER_LAT  CENTER_LON  \n",
       "0              2006   34.220220 -118.620669  \n",
       "1              2010   34.220039 -118.620668  \n",
       "2              2018   34.219858 -118.620676  \n",
       "3              1980   34.220334 -118.622706  \n",
       "4              1984   34.220323 -118.623050  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original data: https://egis-lacounty.hub.arcgis.com/datasets/parcels\n",
    "# this code was used to read in the data and save a subset (City of LA only, subset of columns) that is manageable in size\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "if 0: # if 0 means this block won't be executed\n",
    "    gdf = gpd.read_file('/Users/adammb/Desktop/LACounty_Parcels.gdb', driver='FileGDB', layer='LACounty_Parcels')\n",
    "    gdf.dropna(subset=['SitusCity'], inplace=True)\n",
    "    gdf = gdf[gdf['SitusCity'].str.startswith('LOS ANGELE')]\n",
    "    cols_to_use = ['APN', 'UseType', 'UseDescription','YearBuilt1', 'Units1','Bedrooms1', 'Bathrooms1', \n",
    "         'SQFTmain1','Roll_LandValue', 'Roll_ImpValue', 'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON']\n",
    "    parceldf = pd.DataFrame(gdf)[cols_to_use]  # drops the geometry column as well\n",
    "    parceldf.to_csv('parcels.csv', index=False)\n",
    "    del gdf   # frees up space\n",
    "\n",
    "parcels = pd.read_csv('parcels.csv')\n",
    "parcels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d9527",
   "metadata": {},
   "source": [
    "Note that the `APN` column in `parcels` has a format that corresponds to three columns in `permits`: `Assessor Book`-`Assessor Page`-`Assessor Parcel`. \n",
    "\n",
    "So the first step is to create this `APN` column in `permits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3a2a6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '***'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ecf216536f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n\u001b[1;32m      3\u001b[0m                    \u001b[0;34m+\u001b[0m \u001b[0mpermits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Assessor Page'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5875\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5877\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     def convert(\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '***'"
     ]
    }
   ],
   "source": [
    "# join\n",
    "permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n",
    "                   + permits['Assessor Page'].astype(int).astype(str).str.zfill(3) + '-'\n",
    "                   + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795b1b8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> What went wrong?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f651f694",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "# first, we need to drop the NaNs. They won't join anyway\n",
    "permits.dropna(subset=['Assessor Book', 'Assessor Page','Assessor Parcel'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bb1367",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '***'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-167dd8f934a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n\u001b[1;32m      3\u001b[0m                    \u001b[0;34m+\u001b[0m \u001b[0mpermits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Assessor Page'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5875\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5877\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     def convert(\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/UDS/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '***'"
     ]
    }
   ],
   "source": [
    "# then we run the code again, but we get another error. \n",
    "permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n",
    "                   + permits['Assessor Page'].astype(int).astype(str).str.zfill(3) + '-'\n",
    "                   + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9060de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assessor Book</th>\n",
       "      <th>Assessor Page</th>\n",
       "      <th>Assessor Parcel</th>\n",
       "      <th># of Accessory Dwelling Units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>2307.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>***</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4652</th>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>***</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>2219.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>***</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12083</th>\n",
       "      <td>2340.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>***</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13982</th>\n",
       "      <td>2653.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>***</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14514</th>\n",
       "      <td>2603.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>***</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Assessor Book  Assessor Page Assessor Parcel  \\\n",
       "1585          2307.0           23.0             ***   \n",
       "4652          5084.0           28.0             ***   \n",
       "5523          2219.0            6.0             ***   \n",
       "12083         2340.0           29.0             ***   \n",
       "13982         2653.0           23.0             ***   \n",
       "14514         2603.0           11.0             ***   \n",
       "\n",
       "       # of Accessory Dwelling Units  \n",
       "1585                             1.0  \n",
       "4652                             1.0  \n",
       "5523                             1.0  \n",
       "12083                            1.0  \n",
       "13982                            6.0  \n",
       "14514                            1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at these rows \n",
    "permits[permits['Assessor Parcel']=='***']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09dc17b6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assessor Book</th>\n",
       "      <th>Assessor Page</th>\n",
       "      <th>Assessor Parcel</th>\n",
       "      <th># of Accessory Dwelling Units</th>\n",
       "      <th>APN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2340.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2340-020-013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5535.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5535-034-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2639-023-005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2276-018-028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4249.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4249-006-016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Assessor Book  Assessor Page Assessor Parcel  \\\n",
       "0         2340.0           20.0             013   \n",
       "1         5535.0           34.0             001   \n",
       "2         2639.0           23.0             005   \n",
       "3         2276.0           18.0             028   \n",
       "4         4249.0            6.0             016   \n",
       "\n",
       "   # of Accessory Dwelling Units           APN  \n",
       "0                            1.0  2340-020-013  \n",
       "1                            1.0  5535-034-001  \n",
       "2                            1.0  2639-023-005  \n",
       "3                            1.0  2276-018-028  \n",
       "4                            1.0  4249-006-016  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, we should drop them\n",
    "permits = permits[permits['Assessor Parcel']!='***']\n",
    "permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n",
    "                   + permits['Assessor Page'].astype(int).astype(str).str.zfill(3) + '-'\n",
    "                   + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))\n",
    "permits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d560429",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question:</strong> What type of join do we want? Left? Right? Inner? Outer? 1:1? 1:many?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223f7c9",
   "metadata": {},
   "source": [
    "Note two things:\n",
    "* We need to keep all of the parcels, even if there isn't a corresponding permit. Otherwise, we can't do any predictionâ€”we'll have dataset where *every* parcel has an ADU. So that implies a left join to the parcels dataframe\n",
    "* We don't want to duplicate parcels. So let's drop any duplicates (on the APN column) in both the permit and parcels dataframes. That will guarantee a 1:1 join\n",
    "\n",
    "There are two ways to drop duplicates: the [pandas `drop_duplicates()` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) is one. But sometimes it's easier to use `groupby`, and then take the first in each group. If there is only one row in a group, it will be returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a47d84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping duplicates: 15735\n",
      "After dropping duplicates: 15348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the permits, take the first row of any duplicates for convenience\n",
    "print('Before dropping duplicates: {}'.format(len(permits)))\n",
    "permits = permits.groupby('APN').first()\n",
    "print('After dropping duplicates: {}'.format(len(permits)))\n",
    "permits.index.is_unique  # make sure the index (APN) is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f767a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping duplicates: 789182\n",
      "After dropping duplicates: 789156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Before dropping duplicates: {}'.format(len(parcels)))\n",
    "parcels = parcels.groupby('APN').first()\n",
    "print('After dropping duplicates: {}'.format(len(parcels)))\n",
    "parcels.index.is_unique  # make sure the index (APN) is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15a248",
   "metadata": {},
   "source": [
    "Now let's do the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2cc88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N parcels: 789156\n",
      "N joined: 15142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UseType</th>\n",
       "      <th>UseDescription</th>\n",
       "      <th>YearBuilt1</th>\n",
       "      <th>Units1</th>\n",
       "      <th>Bedrooms1</th>\n",
       "      <th>Bathrooms1</th>\n",
       "      <th>SQFTmain1</th>\n",
       "      <th>Roll_LandValue</th>\n",
       "      <th>Roll_ImpValue</th>\n",
       "      <th>Roll_LandBaseYear</th>\n",
       "      <th>Roll_ImpBaseYear</th>\n",
       "      <th>CENTER_LAT</th>\n",
       "      <th>CENTER_LON</th>\n",
       "      <th>Assessor Book</th>\n",
       "      <th>Assessor Page</th>\n",
       "      <th>Assessor Parcel</th>\n",
       "      <th># of Accessory Dwelling Units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-001-003</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2090.0</td>\n",
       "      <td>543000.0</td>\n",
       "      <td>231000.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006</td>\n",
       "      <td>34.220220</td>\n",
       "      <td>-118.620669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-004</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2479.0</td>\n",
       "      <td>345587.0</td>\n",
       "      <td>238650.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>34.220039</td>\n",
       "      <td>-118.620668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-005</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>490917.0</td>\n",
       "      <td>185207.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>34.219858</td>\n",
       "      <td>-118.620676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-008</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2423.0</td>\n",
       "      <td>119775.0</td>\n",
       "      <td>207020.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>34.220334</td>\n",
       "      <td>-118.622706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-009</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>130511.0</td>\n",
       "      <td>195871.0</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>34.220323</td>\n",
       "      <td>-118.623050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  UseType UseDescription  YearBuilt1  Units1  Bedrooms1  \\\n",
       "APN                                                                       \n",
       "2004-001-003  Residential         Single      1973.0     1.0        4.0   \n",
       "2004-001-004  Residential         Single      1973.0     1.0        5.0   \n",
       "2004-001-005  Residential         Single      1973.0     1.0        4.0   \n",
       "2004-001-008  Residential         Single      1978.0     1.0        4.0   \n",
       "2004-001-009  Residential         Single      1978.0     1.0        4.0   \n",
       "\n",
       "              Bathrooms1  SQFTmain1  Roll_LandValue  Roll_ImpValue  \\\n",
       "APN                                                                  \n",
       "2004-001-003         3.0     2090.0        543000.0       231000.0   \n",
       "2004-001-004         3.0     2479.0        345587.0       238650.0   \n",
       "2004-001-005         2.0     2057.0        490917.0       185207.0   \n",
       "2004-001-008         3.0     2423.0        119775.0       207020.0   \n",
       "2004-001-009         3.0     2226.0        130511.0       195871.0   \n",
       "\n",
       "              Roll_LandBaseYear  Roll_ImpBaseYear  CENTER_LAT  CENTER_LON  \\\n",
       "APN                                                                         \n",
       "2004-001-003               2006              2006   34.220220 -118.620669   \n",
       "2004-001-004               2010              2010   34.220039 -118.620668   \n",
       "2004-001-005               2018              2018   34.219858 -118.620676   \n",
       "2004-001-008               1980              1980   34.220334 -118.622706   \n",
       "2004-001-009               1984              1984   34.220323 -118.623050   \n",
       "\n",
       "              Assessor Book  Assessor Page Assessor Parcel  \\\n",
       "APN                                                          \n",
       "2004-001-003            NaN            NaN             NaN   \n",
       "2004-001-004            NaN            NaN             NaN   \n",
       "2004-001-005            NaN            NaN             NaN   \n",
       "2004-001-008            NaN            NaN             NaN   \n",
       "2004-001-009            NaN            NaN             NaN   \n",
       "\n",
       "              # of Accessory Dwelling Units  \n",
       "APN                                          \n",
       "2004-001-003                            NaN  \n",
       "2004-001-004                            NaN  \n",
       "2004-001-005                            NaN  \n",
       "2004-001-008                            NaN  \n",
       "2004-001-009                            NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedDf = parcels.join(permits, how='left')\n",
    "print('N parcels: {}'.format(len(joinedDf)))\n",
    "print('N joined: {}'.format(joinedDf['# of Accessory Dwelling Units'].count()))\n",
    "joinedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167ac26",
   "metadata": {},
   "source": [
    "That seems good enough. We join almost all of the permits to the parcels dataframe. \n",
    "\n",
    "Now let's create a column that is 0 if there is no ADU (i.e., if the permit data did not join), and 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95528631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UseType</th>\n",
       "      <th>UseDescription</th>\n",
       "      <th>YearBuilt1</th>\n",
       "      <th>Units1</th>\n",
       "      <th>Bedrooms1</th>\n",
       "      <th>Bathrooms1</th>\n",
       "      <th>SQFTmain1</th>\n",
       "      <th>Roll_LandValue</th>\n",
       "      <th>Roll_ImpValue</th>\n",
       "      <th>Roll_LandBaseYear</th>\n",
       "      <th>Roll_ImpBaseYear</th>\n",
       "      <th>CENTER_LAT</th>\n",
       "      <th>CENTER_LON</th>\n",
       "      <th>Assessor Book</th>\n",
       "      <th>Assessor Page</th>\n",
       "      <th>Assessor Parcel</th>\n",
       "      <th># of Accessory Dwelling Units</th>\n",
       "      <th>hasADU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-001-003</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2090.0</td>\n",
       "      <td>543000.0</td>\n",
       "      <td>231000.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006</td>\n",
       "      <td>34.220220</td>\n",
       "      <td>-118.620669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-004</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2479.0</td>\n",
       "      <td>345587.0</td>\n",
       "      <td>238650.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>34.220039</td>\n",
       "      <td>-118.620668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-005</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>490917.0</td>\n",
       "      <td>185207.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>34.219858</td>\n",
       "      <td>-118.620676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-008</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2423.0</td>\n",
       "      <td>119775.0</td>\n",
       "      <td>207020.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>34.220334</td>\n",
       "      <td>-118.622706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-001-009</th>\n",
       "      <td>Residential</td>\n",
       "      <td>Single</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>130511.0</td>\n",
       "      <td>195871.0</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>34.220323</td>\n",
       "      <td>-118.623050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  UseType UseDescription  YearBuilt1  Units1  Bedrooms1  \\\n",
       "APN                                                                       \n",
       "2004-001-003  Residential         Single      1973.0     1.0        4.0   \n",
       "2004-001-004  Residential         Single      1973.0     1.0        5.0   \n",
       "2004-001-005  Residential         Single      1973.0     1.0        4.0   \n",
       "2004-001-008  Residential         Single      1978.0     1.0        4.0   \n",
       "2004-001-009  Residential         Single      1978.0     1.0        4.0   \n",
       "\n",
       "              Bathrooms1  SQFTmain1  Roll_LandValue  Roll_ImpValue  \\\n",
       "APN                                                                  \n",
       "2004-001-003         3.0     2090.0        543000.0       231000.0   \n",
       "2004-001-004         3.0     2479.0        345587.0       238650.0   \n",
       "2004-001-005         2.0     2057.0        490917.0       185207.0   \n",
       "2004-001-008         3.0     2423.0        119775.0       207020.0   \n",
       "2004-001-009         3.0     2226.0        130511.0       195871.0   \n",
       "\n",
       "              Roll_LandBaseYear  Roll_ImpBaseYear  CENTER_LAT  CENTER_LON  \\\n",
       "APN                                                                         \n",
       "2004-001-003               2006              2006   34.220220 -118.620669   \n",
       "2004-001-004               2010              2010   34.220039 -118.620668   \n",
       "2004-001-005               2018              2018   34.219858 -118.620676   \n",
       "2004-001-008               1980              1980   34.220334 -118.622706   \n",
       "2004-001-009               1984              1984   34.220323 -118.623050   \n",
       "\n",
       "              Assessor Book  Assessor Page Assessor Parcel  \\\n",
       "APN                                                          \n",
       "2004-001-003            NaN            NaN             NaN   \n",
       "2004-001-004            NaN            NaN             NaN   \n",
       "2004-001-005            NaN            NaN             NaN   \n",
       "2004-001-008            NaN            NaN             NaN   \n",
       "2004-001-009            NaN            NaN             NaN   \n",
       "\n",
       "              # of Accessory Dwelling Units  hasADU  \n",
       "APN                                                  \n",
       "2004-001-003                            NaN       0  \n",
       "2004-001-004                            NaN       0  \n",
       "2004-001-005                            NaN       0  \n",
       "2004-001-008                            NaN       0  \n",
       "2004-001-009                            NaN       0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are using the '# of Accessory Dwelling Units' column, but any column would work fine\n",
    "joinedDf['hasADU'] = joinedDf['# of Accessory Dwelling Units'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "joinedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073da8a",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "In almost any prediction problem, we want to split the data into a \"training sample\" and a \"testing sample\". Assuming you have a large enough sample, this helps to evaluate the performance of your machine learning model. Essentially, you are assessing how well the model performs against new data that it has not seen before.\n",
    "\n",
    "`scikit-learn` has built in functions to split the data. Note that this chooses a random sample. If you want to replicate your analysis, the `random_state` argument can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10dd46dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3100b1c",
   "metadata": {},
   "source": [
    "At this point, we also need to choose our variables. Our `y` variable is whether there is an ADU or not: `hasADU`.\n",
    "\n",
    "For our `x` (predictor) variables, let's start with a few: `['SQFTmain1', 'Roll_LandValue', 'Roll_ImpValue']`.\n",
    "\n",
    "We also need to drop Null values. Some `pandas` and `numpy` functions do this automatically, but the `scikit-learn` functions normally require us to drop the Null values ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd51b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581968 581968\n",
      "193990 193990\n"
     ]
    }
   ],
   "source": [
    "xvars = ['SQFTmain1', 'Roll_LandValue', 'Roll_ImpValue']\n",
    "yvar = 'hasADU'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = joinedDf[xvars+[yvar]].dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_to_fit[xvars], df_to_fit[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "# check we have a reasonable split\n",
    "print(len(X_train), len(y_train) )\n",
    "print(len(X_test), len(y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159dd914",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Now we are ready to do some machine learning!\n",
    "\n",
    "Random forests are based on the idea of a decision tree. At the crudest level, imagine a classifier that predicts `hasADU=True` if the lot is residential and larger than 5,000sf, and `hasADU=False` otherwise. (The exact threshold such as 5,000sf is estimated from the data.)\n",
    "\n",
    "Here's the example from the Reades et al. article about urban gentrification.\n",
    "\n",
    "<img src=\"10.1177_0042098018789054-fig1.jpg\" width=\"600\">\n",
    "\n",
    "Such a deterministic tree is unlikely to do a great job in prediction. Enter random forests, which are a collection of decision trees. Each tree is trained on a subsample of the data. Then the predictions are combined, rather like the \"wisdom of crowds.\" Formally, this process is called *ensemble learning*.\n",
    "\n",
    "[For a more in-depth explanation, see this article by Neil Liberman.](https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991)\n",
    "\n",
    "Random forests are implemented in `scikit-learn`. An important parameter is the `n_estimators` (i.e., the number of trees). Up to a point, more trees will mean better predictions. But more trees also mean that the code will run more slowly. A general rule: get it working with a small number of trees.\n",
    "\n",
    "Even with 50 trees, you might hear your computer fan run a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09d5397",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2e3c998a8151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# now fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # note there is also a RandomForestRegressor\n",
    "\n",
    "# initialize the random forest classifer object\n",
    "rf = RandomForestClassifier(n_estimators = 50, random_state = 1)\n",
    "\n",
    "# now fit the model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17729548",
   "metadata": {},
   "source": [
    "Once we've estimated the model, we can apply it to predict whether an ADU will be built on an \"unseen\" parcel. In other words, we apply the predictions to the *test* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f041d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca3bbe",
   "metadata": {},
   "source": [
    "How can we assess the performance of this model? One quick check is to look at the means (i.e., the percentage `True`) in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b45fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted fraction True: {:.4f}. Actual fraction True: {:.4f}'.format(y_pred.mean(), y_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df69713",
   "metadata": {},
   "source": [
    "So that doesn't look too good. We can also look more formally at the predictive accuracy.\n",
    "\n",
    "`scikit-learn` has several useful functions here:\n",
    "* The [*confusion matrix*](https://en.wikipedia.org/wiki/Confusion_matrix) gives the number of observations that fall into each predicted and actual category (e.g is True and predicted to be True - a \"true positive\")\n",
    "* The [*accuracy score*](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) gives the fraction of accurate predictions. A high score is no guarantee of a good model. Suppose you want to predict whether a person will be struck by lightening tomorrow. Then just predict \"No\" for everyone. Accurate, but not useful!\n",
    "* The [*classification report*](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) reports key metrics for each category (in this case, `True` and `False`. *Precision* gives the fraction of true positives (TP / (TP + FP). *Recall* gives the fraction of true positives (TP / (TP + FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5167f73",
   "metadata": {},
   "source": [
    "We can also plot the confusion matrix. Note the colorbar shows the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25482f21",
   "metadata": {},
   "source": [
    "That was a useful proof-of-concept. But our predictions were pretty dire. We might be able to fix this by:\n",
    "* Adding more variables\n",
    "* Adding more trees\n",
    "* Adjusting other hyperparameters (we won't discuss that here)\n",
    "\n",
    "Let's throw all the variables in our dataset into the model. This would be bad practice for a hypothesis-testing oriented statistical model. But with prediction, we can throw in the kitchen sink, and rely on the model to separate out what is useful.\n",
    "\n",
    "Note that string variables need to be encoded into dummy variables before we can use them. For example, our `UseType` and `UseDescription` variables need to be split into a series of True or False variables such as `Residential`, `Commercial`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e04d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDf.UseType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2399ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDf.UseDescription.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies1 = pd.get_dummies(joinedDf.UseType, prefix='usetype_')  # creates a dataframe of dummies\n",
    "dummies2 = pd.get_dummies(joinedDf.UseDescription, prefix='usedesc_')\n",
    "dummies1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join back to the original dataframe\n",
    "joinedDf = joinedDf.join(dummies1).join(dummies2) \n",
    "joinedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ac26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = (dummies1.columns.tolist() + dummies2.columns.tolist() + \n",
    "            ['YearBuilt1', 'Units1', 'Bedrooms1', 'Bathrooms1', 'SQFTmain1', 'Roll_LandValue', \n",
    "             'Roll_ImpValue', 'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON' ])\n",
    "yvar = 'hasADU'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = joinedDf[xvars+[yvar]].dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_to_fit[xvars], df_to_fit[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 1, n_jobs=-1) # n_jobs uses all your computer's cores\n",
    "\n",
    "# now fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63603e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98231cc",
   "metadata": {},
   "source": [
    "A bit better. In particular, we do a much better job in getting the true positives (see our precision score). \n",
    "\n",
    "It's still not great. But this is not surprising: ADU construction will depend on many factors that are outside our dataset: lot shape, the ability of the owners to afford an ADU, and their personal circumstances (e.g. having an elderly family member). Bringing census data and other predictors might help.\n",
    "\n",
    "But even if the predictions are not always accurate, the predicted *probabilities* can be informative. We could even map them to indicate where ADUs are likely to be built. \n",
    "\n",
    "We can access the probabilities from the `rf` object. Note it gives us a matrix: the first column is the probability of `False`, and the second is the probability of `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48374fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "predictions = pd.DataFrame(rf.predict_proba(X_test), columns = ['pred_noADU', 'pred_ADU'])\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-advocacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join will join on the index. But X_test has our original index. predictions has an integer index. \n",
    "# reset_index() will create a new integer index for X_test\n",
    "predictions= predictions.join(X_test.reset_index)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a geometry column\n",
    "import geopandas as gpd\n",
    "predictions = gpd.GeoDataFrame(predictions, geometry = gpd.points_from_xy(predictions.CENTER_LON, predictions.CENTER_LAT, crs='EPSG:4326'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cf61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "predictions.to_crs('EPSG:3857').plot('pred_ADU', markersize=0.001, ax=ax)\n",
    "ctx.add_basemap(ax=ax)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d97786",
   "metadata": {},
   "source": [
    "Actually, this map says more about where parcels overlap rather than the presence of an ADU. There are a couple of strategies that we might use to avoid this:\n",
    "* Don't plot parcels with less than a certain probability of having an ADU\n",
    "* Use a different colormap (the `cmap` argument)\n",
    "* Use a smaller marker size (to avoid some overlap) (the `markersize` argument)\n",
    "* Scale the colormap so that it tops out at a lower value (the `vmax` argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c613e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "predictions[predictions.pred_ADU>0.01].to_crs('EPSG:3857').plot('pred_ADU', cmap='Purples', markersize=0.001, \n",
    "                                     vmax=0.02, ax=ax, legend=True)\n",
    "#ctx.add_basemap(ax=ax, alpha = 0.3)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm. Maybe the markersize is a better way to go than a color\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scale the column so we get a smaller markersize\n",
    "predictions['pred_ADU_scaled'] = predictions.pred_ADU*0.2\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "predictions[predictions.pred_ADU>0.01].to_crs('EPSG:3857').plot(cmap='Purples', \n",
    "                                     markersize='pred_ADU_scaled', \n",
    "                                     ax=ax, legend=True)\n",
    "ctx.add_basemap(ax=ax, alpha=0.4)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b6cd7",
   "metadata": {},
   "source": [
    "It's still not clear how to distinguish areas with lots of parcels from areas with few parcels but a high propensity to have an ADU. Any ideas? \n",
    "\n",
    "We'll leave the mapping here, but my inclination would be to aggregate to a grid or to census block groups, in order to make the propensity to have an ADU a bit clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e2914",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question:</strong> How might we improve these predictions?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc306b3c",
   "metadata": {},
   "source": [
    "### What are the factors that affect ADUs\n",
    "In a regression model, the size of the coefficient is a good guide to how important a factor is in determining the outcome.\n",
    "\n",
    "In a random forests setting, the equivalent concept is *feature importance*. It is often measured through \"mean decrease in impurity\" (MDI). The meaning of MDI is beyond our scope here, but you can interpret feature importance as \"how much does a variable improve our predictions.\" \n",
    "\n",
    "The importances are accessed from the `feature_importances_` property of the `rf` object. [The `scikit-learn` docs provide easy-to-implement examples](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html), which we'll follow here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "import numpy as np\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# convert to a series, and give the index labels from our X_train dataframe\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# get the standard deviations to be able to plot the error bars\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# plot importances\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052df79a",
   "metadata": {},
   "source": [
    "So we see that only a few variables are driving the results.\n",
    "\n",
    "The bar chart is pretty bad. We have a few options:\n",
    "* Make a larger plot with horizontal bars, so that the text is easier to read\n",
    "* Only plot the most important features\n",
    "* Use an interactive tool where you can hover over the bar. The `plotly` and `mplcursors` libraries are two options here.\n",
    "\n",
    "Let's try the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025091c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn rather than matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# sort the importances in descending order\n",
    "forest_importances.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,15))\n",
    "sns.barplot(x=forest_importances.values, y=forest_importances.index, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1fc3f",
   "metadata": {},
   "source": [
    "### Unpacking the decision trees\n",
    "Finally, let's take a closer look at the decision trees. Remember, the random forests approach works by estimating an ensemble of trees, and then combining them to get the aggregate prediction.\n",
    "\n",
    "Note that the previous model we estimated is too complex to plot, so let's make it simpler using the `max_depth` hyperparameter. That limits the number of levels of each decision tree.\n",
    "\n",
    "How do we read this tree?\n",
    "* The first line of each box is the decision criterion\n",
    "* *Samples* gives the number of observations at each node on the tree\n",
    "* *Values* gives the outcome (*y*) values, i.e. the number of parcels with and without an ADU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before, but with max_depth=4\n",
    "#rf = RandomForestClassifier(n_estimators = 100, random_state = 1, n_jobs=-1, max_depth=4) \n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "# now we have 100 estimators (trees), so let's pick the first one to visualize\n",
    "#e = rf.estimators_[0]\n",
    "\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(40,20))  \n",
    "_ = tree.plot_tree(e, feature_names = X_train.columns, fontsize=13, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd3f0a",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "Now we've got the syntax nailed down, we can explore other machine learning algorithms.\n",
    "\n",
    "Most follow a similar format, but have different hyperparameters. For the neural network, for example, we can specify the hidden layers.\n",
    "\n",
    "However, many other algorithms are more robust if we *standardize* the data - subtract the mean and divide by the standard deviation. This puts each variable on a common scale.\n",
    "\n",
    "Let's do this. The complication is that we have to exclude the dummy variable columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e87ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://scikit-learn.org/stable/modules/preprocessing.html for standardization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dummyCols = [col for col in df_to_fit.columns if col.startswith('_usetype') or col.startswith('usedesc_') or col=='hasADU']\n",
    "otherCols = [col for col in df_to_fit.columns if col not in dummyCols]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df_to_fit[otherCols])\n",
    "\n",
    "# the scaler returns a numpy array, so we cast this as a DataFrame and need to specify the column names and index\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_to_fit[otherCols]), columns=otherCols, index=df_to_fit.index)\n",
    "df_scaled = df_scaled.join(df_to_fit[dummyCols])\n",
    "\n",
    "df_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba33fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the standardization works\n",
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test as before\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[xvars], df_scaled[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mlp, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65db07",
   "metadata": {},
   "source": [
    "Interestingly, we get very similar results. Perhaps this indicates the inherent unpredictability of ADUs, given how rarely they are constructed. Or we might be able to do better with additional predictors or through adjusting the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fd413",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "As a point of comparison, how would a more traditional logistic regression fare?\n",
    "\n",
    "Many different regression estimators are implemented in `scikit-learn`. And the syntax should be familiar by now. Note that standardization (as we did for neural networks) helps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61237a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cef0fc",
   "metadata": {},
   "source": [
    "Note that it doesn't even converge! Methods like logistic regression don't handle highly correlated variables very well.\n",
    "\n",
    "We might be able to do better with a smaller set of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['YearBuilt1', 'Units1', 'Bedrooms1', 'Bathrooms1', 'SQFTmain1', 'Roll_LandValue', \n",
    "             'Roll_ImpValue', 'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON', 'usedesc__Single']\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train[xvars], y_train)\n",
    "plot_confusion_matrix(lr, X_test[xvars], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c65f60",
   "metadata": {},
   "source": [
    "Not so great, eh? So our random forests and neural networks approaches look much better by comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93785bc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Machine learning is particularly valuable for prediction, and when there are many highly correlated variables.</li>\n",
    "  <li>There are several different approaches. Random forests and neural networks are two of the most popular.</li>\n",
    "  <li>scikit-learn provides a consistent syntax: initialize-fit-predict. So once you've done one ML model, others are much simpler.</li>\n",
    "  <li>Confusion matrices are an excellent way to assess predictive performance.</li>\n",
    "    <li>Feature importance is a way of showing which variables matter in your model.</li>\n",
    "  <li>We focused here on a binary outcome. But ML also works for continuous outcomes (e.g. air quality) and data with multiple categories (e.g. mode choice).</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f0942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
